{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13177225,"sourceType":"datasetVersion","datasetId":8350214}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Install compatible version combinations\n!pip install scikit-learn==1.3.0 imbalanced-learn==0.11.0 --quiet","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-01T03:18:19.386201Z","iopub.execute_input":"2025-10-01T03:18:19.386518Z","iopub.status.idle":"2025-10-01T03:18:19.404805Z","shell.execute_reply.started":"2025-10-01T03:18:19.386496Z","shell.execute_reply":"2025-10-01T03:18:19.403071Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/104403419.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Check package version (optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"✓ imbalanced-learn version: {imblearn.__version__}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# process, as it may not be compiled yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     from . import (\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mcombine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mensemble\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/combine/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_enn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTEENN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_tomek\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTETomek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/combine/_smote_enn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseOverSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneToOneFeatureMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMETHODS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.utils._metadata_requests'"],"ename":"ModuleNotFoundError","evalue":"No module named 'sklearn.utils._metadata_requests'","output_type":"error"}],"execution_count":10},{"cell_type":"code","source":"# ============================================================================\n# Complete Clinical Trial Stroke Risk Prediction Pipeline\n# Part 1: Synthetic Data Generation (CDISC Standards)\n# Part 2: Enhanced ML Model with SMOTE + Cross-Validation\n# ============================================================================\n\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta\nimport random\nfrom sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import (classification_report, roc_auc_score, roc_curve, \n                            confusion_matrix, precision_recall_curve, f1_score,\n                            recall_score, precision_score, fbeta_score)\nfrom imblearn.over_sampling import SMOTE  # For handling class imbalance\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\nnp.random.seed(42)\nrandom.seed(42)\n\nprint(\"=\"*80)\nprint(\"CLINICAL TRIAL STROKE RISK PREDICTION - COMPLETE PIPELINE\")\nprint(\"=\"*80)\nprint(\"Part 1: Synthetic Data Generation (CDISC Standards)\")\nprint(\"Part 2: Enhanced ML Model (SMOTE + 5-Fold CV)\")\nprint(\"=\"*80 + \"\\n\")\n\n# ============================================================================\n# PART 1: SYNTHETIC DATA GENERATION\n# ============================================================================\n\n# 1. Generate Demographics (DM) Table\ndef generate_demographics(n_subjects=1000):\n    \"\"\"Generate demographic data following CDISC DM domain\"\"\"\n    subjects = []\n    for i in range(1, n_subjects + 1):\n        subject = {\n            'SUBJID': f'SUBJ{i:04d}',\n            'AGE': np.random.normal(65, 12),\n            'SEX': np.random.choice(['M', 'F'], p=[0.48, 0.52]),\n            'RACE': np.random.choice(['WHITE', 'BLACK', 'ASIAN', 'HISPANIC', 'OTHER'],\n                                   p=[0.6, 0.15, 0.15, 0.08, 0.02]),\n            'ARM': np.random.choice(['TREATMENT', 'CONTROL'], p=[0.5, 0.5]),\n            'RFSTDTC': datetime(2023, 1, 1) + timedelta(days=np.random.randint(0, 365))\n        }\n        subject['AGE'] = max(18, min(95, subject['AGE']))\n        subjects.append(subject)\n    \n    df_dm = pd.DataFrame(subjects)\n    df_dm['AGE'] = df_dm['AGE'].round(0).astype(int)\n    print(f\"✓ Generated demographics data for {len(df_dm)} subjects\")\n    return df_dm\n\n# 2. Generate Vital Signs (VS) Table\ndef generate_vital_signs(df_dm, visits_per_subject=6):\n    \"\"\"Generate vital signs data following CDISC VS domain\"\"\"\n    vs_records = []\n    for _, subject in df_dm.iterrows():\n        subjid = subject['SUBJID']\n        age = subject['AGE']\n        sex = subject['SEX']\n        \n        base_sbp = 120 + (age - 50) * 0.5 + (10 if sex == 'M' else 0)\n        base_dbp = 80 + (age - 50) * 0.3 + (5 if sex == 'M' else 0)\n        base_hr = 70 + np.random.normal(0, 5)\n        base_temp = 36.5 + np.random.normal(0, 0.3)\n        base_weight = 70 + (10 if sex == 'M' else -5) + np.random.normal(0, 8)\n        \n        for visit in range(1, visits_per_subject + 1):\n            visit_date = subject['RFSTDTC'] + timedelta(days=visit * 30)\n            trend_factor = 1 + (visit - 1) * 0.02\n            \n            vs_records.extend([\n                {\n                    'SUBJID': subjid,\n                    'VSTESTCD': 'SYSBP',\n                    'VSTEST': 'Systolic Blood Pressure',\n                    'VSORRES': max(90, base_sbp * trend_factor + np.random.normal(0, 8)),\n                    'VSORRESU': 'mmHg',\n                    'VSDTC': visit_date,\n                    'VISIT': f'VISIT_{visit}'\n                },\n                {\n                    'SUBJID': subjid,\n                    'VSTESTCD': 'DIABP',\n                    'VSTEST': 'Diastolic Blood Pressure',\n                    'VSORRES': max(60, base_dbp * trend_factor + np.random.normal(0, 5)),\n                    'VSORRESU': 'mmHg',\n                    'VSDTC': visit_date,\n                    'VISIT': f'VISIT_{visit}'\n                },\n                {\n                    'SUBJID': subjid,\n                    'VSTESTCD': 'HR',\n                    'VSTEST': 'Heart Rate',\n                    'VSORRES': max(50, base_hr + np.random.normal(0, 8)),\n                    'VSORRESU': 'beats/min',\n                    'VSDTC': visit_date,\n                    'VISIT': f'VISIT_{visit}'\n                },\n                {\n                    'SUBJID': subjid,\n                    'VSTESTCD': 'TEMP',\n                    'VSTEST': 'Temperature',\n                    'VSORRES': base_temp + np.random.normal(0, 0.2),\n                    'VSORRESU': 'C',\n                    'VSDTC': visit_date,\n                    'VISIT': f'VISIT_{visit}'\n                },\n                {\n                    'SUBJID': subjid,\n                    'VSTESTCD': 'WEIGHT',\n                    'VSTEST': 'Weight',\n                    'VSORRES': max(40, base_weight + np.random.normal(0, 2)),\n                    'VSORRESU': 'kg',\n                    'VSDTC': visit_date,\n                    'VISIT': f'VISIT_{visit}'\n                }\n            ])\n    \n    df_vs = pd.DataFrame(vs_records)\n    df_vs['VSORRES'] = df_vs['VSORRES'].round(1)\n    print(f\"✓ Generated {len(df_vs)} vital signs records\")\n    return df_vs\n\n# 3. Generate Laboratory (LB) Table\ndef generate_lab_data(df_dm, visits_per_subject=6):\n    \"\"\"Generate laboratory data following CDISC LB domain\"\"\"\n    lb_records = []\n    for _, subject in df_dm.iterrows():\n        subjid = subject['SUBJID']\n        age = subject['AGE']\n        \n        base_glucose = 100 + np.random.normal(0, 15)\n        base_cholesterol = 200 + (age - 50) * 1.5 + np.random.normal(0, 30)\n        base_hdl = 50 + np.random.normal(0, 10)\n        base_ldl = 130 + np.random.normal(0, 25)\n        base_triglycerides = 150 + np.random.normal(0, 40)\n        base_creatinine = 1.0 + np.random.normal(0, 0.2)\n        \n        for visit in range(1, visits_per_subject + 1):\n            visit_date = subject['RFSTDTC'] + timedelta(days=visit * 30)\n            \n            lb_records.extend([\n                {\n                    'SUBJID': subjid,\n                    'LBTESTCD': 'GLUCOSE',\n                    'LBTEST': 'Glucose',\n                    'LBORRES': max(70, base_glucose + np.random.normal(0, 10)),\n                    'LBORRESU': 'mg/dL',\n                    'LBDTC': visit_date,\n                    'VISIT': f'VISIT_{visit}'\n                },\n                {\n                    'SUBJID': subjid,\n                    'LBTESTCD': 'CHOL',\n                    'LBTEST': 'Total Cholesterol',\n                    'LBORRES': max(120, base_cholesterol + np.random.normal(0, 15)),\n                    'LBORRESU': 'mg/dL',\n                    'LBDTC': visit_date,\n                    'VISIT': f'VISIT_{visit}'\n                },\n                {\n                    'SUBJID': subjid,\n                    'LBTESTCD': 'HDL',\n                    'LBTEST': 'HDL Cholesterol',\n                    'LBORRES': max(25, base_hdl + np.random.normal(0, 8)),\n                    'LBORRESU': 'mg/dL',\n                    'LBDTC': visit_date,\n                    'VISIT': f'VISIT_{visit}'\n                },\n                {\n                    'SUBJID': subjid,\n                    'LBTESTCD': 'LDL',\n                    'LBTEST': 'LDL Cholesterol',\n                    'LBORRES': max(50, base_ldl + np.random.normal(0, 20)),\n                    'LBORRESU': 'mg/dL',\n                    'LBDTC': visit_date,\n                    'VISIT': f'VISIT_{visit}'\n                },\n                {\n                    'SUBJID': subjid,\n                    'LBTESTCD': 'TRIG',\n                    'LBTEST': 'Triglycerides',\n                    'LBORRES': max(50, base_triglycerides + np.random.normal(0, 30)),\n                    'LBORRESU': 'mg/dL',\n                    'LBDTC': visit_date,\n                    'VISIT': f'VISIT_{visit}'\n                },\n                {\n                    'SUBJID': subjid,\n                    'LBTESTCD': 'CREAT',\n                    'LBTEST': 'Creatinine',\n                    'LBORRES': max(0.5, base_creatinine + np.random.normal(0, 0.1)),\n                    'LBORRESU': 'mg/dL',\n                    'LBDTC': visit_date,\n                    'VISIT': f'VISIT_{visit}'\n                }\n            ])\n    \n    df_lb = pd.DataFrame(lb_records)\n    df_lb['LBORRES'] = df_lb['LBORRES'].round(2)\n    print(f\"✓ Generated {len(df_lb)} laboratory records\")\n    return df_lb\n\n# 4. Generate Stroke Outcomes\ndef generate_stroke_outcomes(df_dm, df_vs, df_lb):\n    \"\"\"Generate stroke outcomes based on clinical risk factors\"\"\"\n    outcomes = []\n    \n    vs_pivot = df_vs.pivot_table(\n        index='SUBJID',\n        columns='VSTESTCD',\n        values='VSORRES',\n        aggfunc='mean'\n    ).reset_index()\n    \n    lb_pivot = df_lb.pivot_table(\n        index='SUBJID',\n        columns='LBTESTCD',\n        values='LBORRES',\n        aggfunc='mean'\n    ).reset_index()\n    \n    merged_data = df_dm.merge(vs_pivot, on='SUBJID', how='left')\n    merged_data = merged_data.merge(lb_pivot, on='SUBJID', how='left')\n    \n    for _, row in merged_data.iterrows():\n        risk_score = 0\n        \n        if row['AGE'] > 65:\n            risk_score += (row['AGE'] - 65) * 0.1\n        \n        if 'SYSBP' in row and pd.notna(row['SYSBP']):\n            if row['SYSBP'] > 140:\n                risk_score += (row['SYSBP'] - 140) * 0.05\n        \n        if 'CHOL' in row and pd.notna(row['CHOL']):\n            if row['CHOL'] > 240:\n                risk_score += (row['CHOL'] - 240) * 0.02\n        \n        if row['SEX'] == 'M':\n            risk_score += 1.5\n        \n        if row['RACE'] == 'BLACK':\n            risk_score += 1.0\n        \n        if 'GLUCOSE' in row and pd.notna(row['GLUCOSE']):\n            if row['GLUCOSE'] > 126:\n                risk_score += 2.0\n        \n        if row['ARM'] == 'TREATMENT':\n            risk_score *= 0.8\n        \n        probability = 1 / (1 + np.exp(-(risk_score - 5)))\n        stroke = np.random.random() < probability\n        \n        outcomes.append({\n            'SUBJID': row['SUBJID'],\n            'STROKE': int(stroke),\n            'RISK_SCORE': round(risk_score, 2),\n            'STROKE_PROBABILITY': round(probability, 3)\n        })\n    \n    df_outcomes = pd.DataFrame(outcomes)\n    stroke_rate = df_outcomes['STROKE'].mean()\n    print(f\"✓ Generated stroke outcomes, stroke rate: {stroke_rate:.1%}\")\n    return df_outcomes\n\n# 5. Generate Adverse Events (AE) Table\ndef generate_adverse_events(df_dm, df_outcomes):\n    \"\"\"Generate adverse events data following CDISC AE domain\"\"\"\n    ae_records = []\n    ae_terms = [\n        'HYPERTENSION', 'DIZZINESS', 'HEADACHE', 'FATIGUE',\n        'NAUSEA', 'CHEST PAIN', 'PALPITATIONS', 'EDEMA',\n        'SHORTNESS OF BREATH', 'MUSCLE CRAMPS'\n    ]\n    ae_severity = ['MILD', 'MODERATE', 'SEVERE']\n    ae_outcomes = ['RECOVERED/RESOLVED', 'RECOVERING/RESOLVING', 'NOT RECOVERED/NOT RESOLVED']\n    \n    for _, subject in df_dm.iterrows():\n        subjid = subject['SUBJID']\n        n_aes = np.random.poisson(2)\n        \n        for ae_num in range(n_aes):\n            ae_start = subject['RFSTDTC'] + timedelta(days=np.random.randint(1, 365))\n            ae_records.append({\n                'SUBJID': subjid,\n                'AESEQ': ae_num + 1,\n                'AETERM': np.random.choice(ae_terms),\n                'AESTDTC': ae_start,\n                'AESEV': np.random.choice(ae_severity, p=[0.6, 0.3, 0.1]),\n                'AEOUT': np.random.choice(ae_outcomes, p=[0.7, 0.2, 0.1]),\n                'AEREL': np.random.choice(['NOT RELATED', 'UNLIKELY', 'POSSIBLE', 'PROBABLE'],\n                                        p=[0.4, 0.3, 0.2, 0.1])\n            })\n    \n    df_ae = pd.DataFrame(ae_records)\n    print(f\"✓ Generated {len(df_ae)} adverse event records\")\n    return df_ae\n\n# 6. Generate Exposure (EX) Table\ndef generate_exposure_data(df_dm):\n    \"\"\"Generate exposure data following CDISC EX domain\"\"\"\n    ex_records = []\n    for _, subject in df_dm.iterrows():\n        subjid = subject['SUBJID']\n        treatment = subject['ARM']\n        \n        if treatment == 'TREATMENT':\n            drug_name = 'STUDY DRUG'\n            dose = '10 mg'\n            route = 'ORAL'\n        else:\n            drug_name = 'PLACEBO'\n            dose = 'N/A'\n            route = 'ORAL'\n        \n        max_duration = 365\n        actual_duration = min(max_duration, np.random.exponential(300))\n        start_date = subject['RFSTDTC']\n        end_date = start_date + timedelta(days=int(actual_duration))\n        \n        ex_records.append({\n            'SUBJID': subjid,\n            'EXTRT': drug_name,\n            'EXDOSE': dose,\n            'EXDOSU': 'mg' if dose != 'N/A' else '',\n            'EXROUTE': route,\n            'EXSTDTC': start_date,\n            'EXENDTC': end_date,\n            'EXDUR': int(actual_duration)\n        })\n    \n    df_ex = pd.DataFrame(ex_records)\n    print(f\"✓ Generated {len(df_ex)} exposure records\")\n    return df_ex\n\n# ============================================================================\n# PART 2: DATA PREPARATION & FEATURE ENGINEERING\n# ============================================================================\n\ndef prepare_modeling_dataset(df_dm, df_vs, df_lb, df_outcomes):\n    \"\"\"Prepare dataset for modeling by merging and aggregating clinical data\"\"\"\n    print(\"\\n\" + \"=\"*80)\n    print(\"PREPARING MODELING DATASET\")\n    print(\"=\"*80)\n    \n    vs_pivot = df_vs.pivot_table(\n        index='SUBJID',\n        columns='VSTESTCD',\n        values='VSORRES',\n        aggfunc='mean'\n    )\n    vs_pivot.columns = [f'VS_{test}' for test in vs_pivot.columns]\n    vs_pivot = vs_pivot.reset_index()\n    \n    lb_pivot = df_lb.pivot_table(\n        index='SUBJID',\n        columns='LBTESTCD',\n        values='LBORRES',\n        aggfunc='mean'\n    )\n    lb_pivot.columns = [f'LB_{test}' for test in lb_pivot.columns]\n    lb_pivot = lb_pivot.reset_index()\n    \n    modeling_data = df_dm.copy()\n    modeling_data = modeling_data.merge(vs_pivot, on='SUBJID', how='left')\n    modeling_data = modeling_data.merge(lb_pivot, on='SUBJID', how='left')\n    modeling_data = modeling_data.merge(df_outcomes, on='SUBJID', how='left')\n    \n    print(f\"✓ Modeling dataset: {len(modeling_data)} subjects, {len(modeling_data.columns)} features\")\n    return modeling_data\n\n# ============================================================================\n# PART 3: ENHANCED MODEL WITH SMOTE + CROSS-VALIDATION\n# ============================================================================\n\ndef build_enhanced_stroke_model(df_modeling, use_smote=True, n_folds=5):\n    \"\"\"\n    Build enhanced model with:\n    - SMOTE for class imbalance\n    - 5-fold cross-validation with confidence intervals\n    - Clinical-focused metrics (prioritizing recall)\n    - 500 trees for efficiency\n    \"\"\"\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"ENHANCED MODEL BUILDING\")\n    print(\"=\"*80)\n    print(f\"SMOTE: {'Enabled' if use_smote else 'Disabled'}\")\n    print(f\"Cross-Validation: {n_folds}-fold\")\n    print(f\"Random Forest Trees: 500\")\n    print(\"=\"*80 + \"\\n\")\n    \n    # Prepare features\n    feature_columns = ['AGE']\n    vs_features = [col for col in df_modeling.columns if col.startswith('VS_')]\n    lab_features = [col for col in df_modeling.columns if col.startswith('LB_')]\n    feature_columns.extend(vs_features + lab_features)\n    feature_columns = [col for col in feature_columns if col in df_modeling.columns]\n    \n    X = df_modeling[feature_columns].copy()\n    y = df_modeling['STROKE'].copy()\n    \n    # Add categorical features\n    for cat_feature in ['SEX', 'RACE', 'ARM']:\n        if cat_feature in df_modeling.columns:\n            dummies = pd.get_dummies(df_modeling[cat_feature], prefix=cat_feature)\n            X = pd.concat([X, dummies], axis=1)\n    \n    X = X.fillna(X.mean())\n    \n    print(f\"Features: {X.shape[1]} total\")\n    print(f\"Class distribution: No Stroke={sum(y==0)}, Stroke={sum(y==1)}\")\n    print(f\"Imbalance ratio: {(1-y.mean())/y.mean():.1f}:1\\n\")\n    \n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    \n    # Apply SMOTE to training set only\n    if use_smote:\n        print(\"Applying SMOTE to training set...\")\n        print(f\"Before SMOTE: No Stroke={sum(y_train==0)}, Stroke={sum(y_train==1)}\")\n        \n        smote = SMOTE(random_state=42, k_neighbors=5)\n        X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n        \n        print(f\"After SMOTE:  No Stroke={sum(y_train_balanced==0)}, Stroke={sum(y_train_balanced==1)}\")\n        print(f\"Created {len(y_train_balanced) - len(y_train)} synthetic stroke cases\\n\")\n    else:\n        X_train_balanced = X_train\n        y_train_balanced = y_train\n    \n    # Initialize models\n    models = {\n        'Logistic Regression': LogisticRegression(\n            random_state=42,\n            max_iter=1000,\n            class_weight='balanced'\n        ),\n        'Random Forest': RandomForestClassifier(\n            n_estimators=500,  # Reduced from 10000\n            random_state=42,\n            max_depth=15,\n            min_samples_split=10,\n            min_samples_leaf=4,\n            class_weight='balanced'\n        )\n    }\n    \n    results = {}\n    \n    # Train and evaluate\n    print(\"Training models...\\n\")\n    for name, model in models.items():\n        print(f\"--- {name} ---\")\n        \n        if name == 'Logistic Regression':\n            scaler = StandardScaler()\n            X_train_proc = scaler.fit_transform(X_train_balanced)\n            X_test_proc = scaler.transform(X_test)\n            model.fit(X_train_proc, y_train_balanced)\n            y_pred_proba = model.predict_proba(X_test_proc)[:, 1]\n            y_pred = model.predict(X_test_proc)\n        else:\n            scaler = None\n            model.fit(X_train_balanced, y_train_balanced)\n            y_pred_proba = model.predict_proba(X_test)[:, 1]\n            y_pred = model.predict(X_test)\n        \n        # Metrics\n        auc_score = roc_auc_score(y_test, y_pred_proba)\n        recall = recall_score(y_test, y_pred)\n        precision = precision_score(y_test, y_pred)\n        f1 = f1_score(y_test, y_pred)\n        f2 = fbeta_score(y_test, y_pred, beta=2)\n        \n        print(f\"AUC: {auc_score:.3f}\")\n        print(f\"Recall (Sensitivity): {recall:.3f} ⭐\")\n        print(f\"Precision: {precision:.3f}\")\n        print(f\"F2 Score: {f2:.3f}\")\n        \n        # Cross-validation\n        skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n        \n        if name == 'Logistic Regression':\n            cv_scores = []\n            for train_idx, val_idx in skf.split(X_train_balanced, y_train_balanced):\n                X_fold_train = X_train_balanced.iloc[train_idx]\n                X_fold_val = X_train_balanced.iloc[val_idx]\n                y_fold_train = y_train_balanced.iloc[train_idx]\n                y_fold_val = y_train_balanced.iloc[val_idx]\n                \n                fold_scaler = StandardScaler()\n                X_fold_train_scaled = fold_scaler.fit_transform(X_fold_train)\n                X_fold_val_scaled = fold_scaler.transform(X_fold_val)\n                \n                model.fit(X_fold_train_scaled, y_fold_train)\n                y_fold_proba = model.predict_proba(X_fold_val_scaled)[:, 1]\n                cv_scores.append(roc_auc_score(y_fold_val, y_fold_proba))\n        else:\n            cv_scores = cross_val_score(\n                model, X_train_balanced, y_train_balanced,\n                cv=skf, scoring='roc_auc', n_jobs=-1\n            )\n        \n        mean_cv = np.mean(cv_scores)\n        std_cv = np.std(cv_scores)\n        ci_lower = mean_cv - 1.96 * std_cv\n        ci_upper = mean_cv + 1.96 * std_cv\n        \n        print(f\"{n_folds}-Fold CV AUC: {mean_cv:.3f} ± {std_cv:.3f}\")\n        print(f\"95% CI: ({ci_lower:.3f}, {ci_upper:.3f})\\n\")\n        \n        results[name] = {\n            'model': model,\n            'scaler': scaler,\n            'auc': auc_score,\n            'recall': recall,\n            'precision': precision,\n            'f1': f1,\n            'f2': f2,\n            'y_pred_proba': y_pred_proba,\n            'y_pred': y_pred,\n            'features': X.columns.tolist(),\n            'cv_mean': mean_cv,\n            'cv_std': std_cv,\n            'cv_ci': (ci_lower, ci_upper)\n        }\n    \n    # Clinical summary\n    print(\"=\"*80)\n    print(\"CLINICAL METRICS SUMMARY\")\n    print(\"=\"*80)\n    print(\"⚠️  False Negative (missed stroke) >> False Positive (false alarm)\\n\")\n    \n    for name, result in results.items():\n        cm = confusion_matrix(y_test, result['y_pred'])\n        tn, fp, fn, tp = cm.ravel()\n        \n        print(f\"{name}:\")\n        print(f\"  Caught strokes: {tp}/{tp+fn} ({tp/(tp+fn):.1%})\")\n        print(f\"  Missed strokes: {fn}/{tp+fn} ({fn/(tp+fn):.1%}) ⚠️\")\n        print(f\"  False alarms: {fp}/{tn+fp} ({fp/(tn+fp):.1%}\")\n        print()\n    \n    return results, X_test, y_test\n\n# ============================================================================\n# PART 4: RISK CALCULATOR\n# ============================================================================\n\ndef create_stroke_risk_calculator(results, df_modeling):\n    \"\"\"Create enhanced risk calculator\"\"\"\n    \n    # Choose best model by recall\n    best_model_name = max(results.keys(), \n                         key=lambda x: (results[x]['recall'], results[x]['auc']))\n    \n    best_result = results[best_model_name]\n    best_model = best_result['model']\n    scaler = best_result['scaler']\n    feature_names = best_result['features']\n    \n    print(\"\\n\" + \"=\"*80)\n    print(f\"RISK CALCULATOR: {best_model_name}\")\n    print(\"=\"*80)\n    print(f\"Test AUC: {best_result['auc']:.3f}\")\n    print(f\"CV AUC: {best_result['cv_mean']:.3f} ± {best_result['cv_std']:.3f}\")\n    print(f\"95% CI: {best_result['cv_ci']}\")\n    print(f\"Recall: {best_result['recall']:.3f} ⭐\")\n    print(\"=\"*80 + \"\\n\")\n    \n    def predict_stroke_risk(age, sex, systolic_bp, glucose, cholesterol,\n                           hdl=50, ldl=130, creatinine=1.0, heart_rate=70,\n                           diastolic_bp=80, race='WHITE', treatment='CONTROL',\n                           debug=False):\n        \"\"\"Calculate stroke risk for a patient\"\"\"\n        \n        input_data = pd.DataFrame({\n            'AGE': [age],\n            'VS_SYSBP': [systolic_bp],\n            'VS_DIABP': [diastolic_bp],\n            'VS_HR': [heart_rate],\n            'LB_CHOL': [cholesterol],\n            'LB_GLUCOSE': [glucose],\n            'LB_HDL': [hdl],\n            'LB_LDL': [ldl],\n            'LB_CREAT': [creatinine]\n        })\n        \n        input_data['SEX_F'] = 1 if sex == 'F' else 0\n        input_data['SEX_M'] = 1 if sex == 'M' else 0\n        \n        for r in ['ASIAN', 'BLACK', 'HISPANIC', 'OTHER', 'WHITE']:\n            input_data[f'RACE_{r}'] = 1 if race == r else 0\n        \n        input_data['ARM_CONTROL'] = 1 if treatment == 'CONTROL' else 0\n        input_data['ARM_TREATMENT'] = 1 if treatment == 'TREATMENT' else 0\n        \n        for feature in feature_names:\n            if feature not in input_data.columns:\n                input_data[feature] = 0\n        input_data = input_data.reindex(columns=feature_names, fill_value=0)\n        \n        try:\n            if scaler is not None:\n                input_proc = scaler.transform(input_data)\n                risk_prob = best_model.predict_proba(input_proc)[0, 1]\n            else:\n                risk_prob = best_model.predict_proba(input_data)[0, 1]\n            \n            if debug:\n                print(f\"\\n{'='*70}\")\n                print(\"PREDICTION DETAILS\")\n                print(f\"{'='*70}\")\n                print(f\"Patient: {age}yo {sex}, SBP={systolic_bp}, Glucose={glucose}, Chol={cholesterol}\")\n                print(f\"Model: {best_model_name}\")\n                print(f\"Raw probability: {risk_prob:.6f}\")\n                print(f\"Stroke risk: {risk_prob*100:.3f}%\")\n                print(f\"Risk category: \", end='')\n                if risk_prob < 0.05:\n                    print(\"🟢 Low risk\")\n                elif risk_prob < 0.15:\n                    print(\"🟡 Moderate risk\")\n                elif risk_prob < 0.25:\n                    print(\"🟠 High risk\")\n                else:\n                    print(\"🔴 Very high risk\")\n                print(f\"{'='*70}\\n\")\n            \n            return risk_prob\n            \n        except Exception as e:\n            print(f\"❌ Error: {e}\")\n            return 0.0\n    \n    return predict_stroke_risk\n\n# ============================================================================\n# PART 5: VISUALIZATION\n# ============================================================================\n\ndef plot_data_quality(df_dm, df_outcomes):\n    \"\"\"Plot data quality and distributions\"\"\"\n    fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n    fig.suptitle('Data Quality Assessment', fontsize=14, fontweight='bold')\n    \n    # Stroke distribution\n    df_outcomes['STROKE'].value_counts().plot(kind='bar', ax=axes[0])\n    axes[0].set_title('Stroke Distribution')\n    axes[0].set_ylabel('Count')\n    axes[0].set_xticklabels(['No Stroke', 'Stroke'], rotation=0)\n    \n    # Risk score distribution\n    axes[1].hist(df_outcomes['RISK_SCORE'], bins=30, alpha=0.7, color='steelblue')\n    axes[1].set_title('Risk Score Distribution')\n    axes[1].set_xlabel('Risk Score')\n    \n    # Treatment groups\n    df_dm['ARM'].value_counts().plot(kind='pie', ax=axes[2], autopct='%1.1f%%')\n    axes[2].set_title('Treatment Groups')\n    axes[2].set_ylabel('')\n    \n    # Stroke by treatment\n    stroke_by_arm = df_dm.merge(df_outcomes, on='SUBJID').groupby('ARM')['STROKE'].mean()\n    stroke_by_arm.plot(kind='bar', ax=axes[3], color=['coral', 'lightblue'])\n    axes[3].set_title('Stroke Rate by Treatment')\n    axes[3].set_ylabel('Stroke Rate')\n    axes[3].set_xticklabels(axes[3].get_xticklabels(), rotation=0)\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_model_evaluation(results, X_test, y_test):\n    \"\"\"Plot comprehensive model evaluation\"\"\"\n    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n    fig.suptitle('Enhanced Model Evaluation', fontsize=16, fontweight='bold')\n    \n    # 1. ROC Curves\n    axes[0,0].plot([0, 1], [0, 1], 'k--', alpha=0.3)\n    for name, result in results.items():\n        fpr, tpr, _ = roc_curve(y_test, result['y_pred_proba'])\n        axes[0,0].plot(fpr, tpr, linewidth=2,\n                      label=f\"{name}\\nAUC={result['auc']:.3f}\\nCV={result['cv_mean']:.3f}±{result['cv_std']:.3f}\")\n    axes[0,0].set_xlabel('False Positive Rate')\n    axes[0,0].set_ylabel('True Positive Rate')\n    axes[0,0].set_title('ROC Curves')\n    axes[0,0].legend(fontsize=9)\n    axes[0,0].grid(True, alpha=0.3)\n    \n    # 2. Precision-Recall\n    for name, result in results.items():\n        precision, recall, _ = precision_recall_curve(y_test, result['y_pred_proba'])\n        axes[0,1].plot(recall, precision, linewidth=2, label=name)\n    axes[0,1].set_xlabel('Recall')\n    axes[0,1].set_ylabel('Precision')\n    axes[0,1].set_title('Precision-Recall Curves')\n    axes[0,1].legend()\n    axes[0,1].grid(True, alpha=0.3)\n    \n    # 3. Metrics comparison\n    metrics = ['AUC', 'Recall', 'Precision', 'F2']\n    x = np.arange(len(metrics))\n    width = 0.35\n    \n    for i, (name, result) in enumerate(results.items()):\n        values = [result['auc'], result['recall'], result['precision'], result['f2']]\n        axes[0,2].bar(x + i*width, values, width, label=name, alpha=0.8)\n    \n    axes[0,2].set_ylabel('Score')\n    axes[0,2].set_title('Clinical Metrics')\n    axes[0,2].set_xticks(x + width/2)\n    axes[0,2].set_xticklabels(metrics)\n    axes[0,2].legend()\n    axes[0,2].set_ylim(0, 1.1)\n    axes[0,2].grid(True, alpha=0.3, axis='y')\n    \n    # 4-5. Confusion matrices\n    for i, (name, result) in enumerate(results.items()):\n        cm = confusion_matrix(y_test, result['y_pred'])\n        tn, fp, fn, tp = cm.ravel()\n        \n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1,i],\n                   xticklabels=['No Stroke', 'Stroke'],\n                   yticklabels=['No Stroke', 'Stroke'])\n        axes[1,i].set_ylabel('Actual')\n        axes[1,i].set_xlabel('Predicted')\n        axes[1,i].set_title(f'{name}\\nRecall={result[\"recall\"]:.1%}, Missed={fn} strokes')\n    \n    # 6. Feature importance (Random Forest)\n    if 'Random Forest' in results:\n        rf_result = results['Random Forest']\n        feature_names = rf_result['features']\n        importance_df = pd.DataFrame({\n            'feature': feature_names,\n            'importance': rf_result['model'].feature_importances_\n        }).sort_values('importance', ascending=False).head(10)\n        \n        axes[1,2].barh(range(len(importance_df)), importance_df['importance'])\n        axes[1,2].set_yticks(range(len(importance_df)))\n        axes[1,2].set_yticklabels(importance_df['feature'], fontsize=9)\n        axes[1,2].invert_yaxis()\n        axes[1,2].set_xlabel('Importance')\n        axes[1,2].set_title('Top 10 Features (RF)')\n        axes[1,2].grid(True, alpha=0.3, axis='x')\n    \n    plt.tight_layout()\n    plt.show()\n\n# ============================================================================\n# MAIN EXECUTION\n# ============================================================================\n\nif __name__ == \"__main__\":\n    print(\"\\nStarting complete pipeline...\\n\")\n    \n    # PART 1: Generate synthetic data\n    print(\"=\"*80)\n    print(\"PART 1: GENERATING SYNTHETIC CLINICAL DATA\")\n    print(\"=\"*80 + \"\\n\")\n    \n    df_dm = generate_demographics(n_subjects=1000)\n    df_vs = generate_vital_signs(df_dm, visits_per_subject=4)\n    df_lb = generate_lab_data(df_dm, visits_per_subject=4)\n    df_outcomes = generate_stroke_outcomes(df_dm, df_vs, df_lb)\n    df_ae = generate_adverse_events(df_dm, df_outcomes)\n    df_ex = generate_exposure_data(df_dm)\n    \n    print(f\"\\n{'='*80}\")\n    print(\"DATA GENERATION COMPLETE\")\n    print(f\"{'='*80}\")\n    print(f\"Demographics: {len(df_dm)} records\")\n    print(f\"Vital Signs: {len(df_vs)} records\")\n    print(f\"Laboratory: {len(df_lb)} records\")\n    print(f\"Adverse Events: {len(df_ae)} records\")\n    print(f\"Exposure: {len(df_ex)} records\")\n    print(f\"Outcomes: {len(df_outcomes)} records\")\n    print(f\"Stroke rate: {df_outcomes['STROKE'].mean():.1%}\")\n    \n    # Save data\n    df_dm.to_csv('demographics.csv', index=False)\n    df_vs.to_csv('vital_signs.csv', index=False)\n    df_lb.to_csv('laboratory.csv', index=False)\n    df_ae.to_csv('adverse_events.csv', index=False)\n    df_ex.to_csv('exposure.csv', index=False)\n    df_outcomes.to_csv('stroke_outcomes.csv', index=False)\n    print(\"\\n✓ All data saved as CSV files\")\n    \n    # Data quality visualization\n    plot_data_quality(df_dm, df_outcomes)\n    \n    # PART 2: Prepare modeling dataset\n    df_modeling = prepare_modeling_dataset(df_dm, df_vs, df_lb, df_outcomes)\n    \n    # PART 3: Build enhanced model\n    results, X_test, y_test = build_enhanced_stroke_model(\n        df_modeling,\n        use_smote=True,  # ⭐ Enable SMOTE\n        n_folds=5        # ⭐ 5-fold CV\n    )\n    \n    # PART 4: Visualize results\n    plot_model_evaluation(results, X_test, y_test)\n    \n    # PART 5: Create risk calculator\n    predict_stroke_risk = create_stroke_risk_calculator(results, df_modeling)\n    \n    # PART 6: Example predictions\n    print(\"\\n\" + \"=\"*80)\n    print(\"EXAMPLE PREDICTIONS\")\n    print(\"=\"*80 + \"\\n\")\n    \n    print(\"Example 1: Moderate-High Risk Patient\")\n    risk1 = predict_stroke_risk(\n        age=72, sex='M', systolic_bp=150,\n        glucose=110, cholesterol=220, debug=True\n    )\n    \n    print(\"Example 2: Very High Risk Patient\")\n    risk2 = predict_stroke_risk(\n        age=80, sex='M', systolic_bp=180,\n        glucose=180, cholesterol=280, debug=True\n    )\n    \n    print(\"Example 3: Low Risk Patient\")\n    risk3 = predict_stroke_risk(\n        age=45, sex='F', systolic_bp=110,\n        glucose=90, cholesterol=180, debug=True\n    )\n    \n    print(\"\\n\" + \"=\"*80)\n    print(\"PIPELINE COMPLETE!\")\n    print(\"=\"*80)\n    print(\"\\nKey Features:\")\n    print(\"✓ Synthetic data generated following CDISC standards\")\n    print(\"✓ SMOTE applied to balance training data\")\n    print(\"✓ 5-fold cross-validation with confidence intervals\")\n    print(\"✓ Clinical metrics prioritizing recall (sensitivity)\")\n    print(\"✓ 500 trees for computational efficiency\")\n    print(\"✓ Risk calculator ready for use\")\n    print(\"\\nYou can now use predict_stroke_risk() for new patients!\")\n    print(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T03:16:36.690202Z","iopub.execute_input":"2025-10-01T03:16:36.690702Z","iopub.status.idle":"2025-10-01T03:16:36.929321Z","shell.execute_reply.started":"2025-10-01T03:16:36.690665Z","shell.execute_reply":"2025-10-01T03:16:36.927739Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/1750756896.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                             \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_recall_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                             recall_score, precision_score, fbeta_score)\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m  \u001b[0;31m# For handling class imbalance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# process, as it may not be compiled yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     from . import (\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mcombine\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mensemble\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/combine/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_enn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTEENN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_smote_tomek\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTETomek\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/combine/_smote_enn.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mover_sampling\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseOverSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/imblearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOneToOneFeatureMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlabel_binarize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMETHODS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulticlass\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.utils._metadata_requests'"],"ename":"ModuleNotFoundError","evalue":"No module named 'sklearn.utils._metadata_requests'","output_type":"error"}],"execution_count":8},{"cell_type":"code","source":"# Sample Test\nrisk = predict_stroke_risk(\n    age=72, \n    sex='M',\n    systolic_bp=150,\n    glucose=110, \n    cholesterol=150,\n    debug=True\n)\nprint(f\"\\nStroke risk: {risk*100:.3f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T03:16:36.929929Z","iopub.status.idle":"2025-10-01T03:16:36.930332Z","shell.execute_reply.started":"2025-10-01T03:16:36.930151Z","shell.execute_reply":"2025-10-01T03:16:36.930170Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Sample Test 2\nrisk = predict_stroke_risk(\n    age=72, \n    sex='M',\n    systolic_bp=180,\n    glucose=180, \n    cholesterol=240,\n    debug=True\n)\nprint(f\"\\nStroke risk: {risk*100:.3f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-01T03:16:36.932481Z","iopub.status.idle":"2025-10-01T03:16:36.932858Z","shell.execute_reply.started":"2025-10-01T03:16:36.932688Z","shell.execute_reply":"2025-10-01T03:16:36.932702Z"}},"outputs":[],"execution_count":null}]}